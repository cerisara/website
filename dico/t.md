## Transformer

Le modèle qui a complètement remplacé toute la famille
des réseaux récurrents (RNN, LSTM, GRU...) par une nouvelle
famille basée sur la self-attention.
Pourquoi les transformers sont meilleurs ?
Parce que les empiler permet de capturer l'information provenant
de corpus textuels de plus en plus gigantesques, tout comme les
CNN avec les gros corpus d'images.
Les transformers ont révolutionné le TAL, et sont à la base de
tous les modèles récents du domaine: BERT, GPT-1.2.3, RoBERTa,
ALBERT, XL-NET, T5, BART...

